{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ec4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from itertools import cycle\n",
    "import random\n",
    "import collections\n",
    "from random import choice\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915ac6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_url = 'https://github.com/saschazesiger/Free-Proxies/blob/master/proxies/ultrafast.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7219cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxies(url):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, 'lxml').find_all('td',{'class':'blob-code blob-code-inner js-file-line'})\n",
    "    proxies = [\"http://\"+i.text for i in soup]\n",
    "\n",
    "    keys = []\n",
    "    for i in range(len(proxies)):\n",
    "        keys.append('http')\n",
    "\n",
    "    proxy_list = [{L2 : L1} for (L1,L2) in zip(proxies, keys)]  \n",
    "    \n",
    "    return proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02e2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_list = get_proxies(proxy_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0cf8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "del proxy_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcfb3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ed3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_check(url, proxy_list):\n",
    "    global proxy_index\n",
    "    '''\n",
    "    #working = []\n",
    "    for i in range(0,100):\n",
    "    while True:\n",
    "        try:\n",
    "            # do stuff\n",
    "        except SomeSpecificException:\n",
    "            continue\n",
    "        break\n",
    "    #for i in range(100):\n",
    "    '''\n",
    "    working = []\n",
    "    while proxy_index < len(proxy_list)/2000:\n",
    "    #for i in proxy_list[:25]:\n",
    "        #proxy = get_random_proxy(new_proxies)\n",
    "        #x = choice(proxy_list)\n",
    "        try:\n",
    "            #print(f\"Using {x} ...\")\n",
    "            print(f\"Using {proxy_list[proxy_index]} ...\")\n",
    "            #time.sleep(random.randint(10,30))\n",
    "            r = requests.get(url ,proxies=proxy_list[proxy_index], timeout=3)\n",
    "            #r = requests.get(url ,proxies=x, timeout=3)\n",
    "            #working.append(proxy_list[proxy_index])\n",
    "            print(r.status_code)\n",
    "        #print(r)\n",
    "            return r\n",
    "        except:\n",
    "            proxy_index += 1\n",
    "            print('Bad proxy..')\n",
    "            print(proxy_index)\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e8ef406",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tech_url = 'https://www.techpowerup.com'\n",
    "test_url = 'https://www.google.com/'\n",
    "\n",
    "#source = proxy_check(tech_url, proxy_list)\n",
    "source = proxy_check(test_url, proxy_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddbf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b54349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_url = 'https://www.google.com/'\n",
    "# 'https://www.techpowerup.com/gpu-specs/'\n",
    "#proxy = choice(proxy_check()) /gpu-specs/\n",
    "\n",
    "source = proxy_check(tech_url, proxy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d82323a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tech_url = 'https://www.techpowerup.com/gpu-specs/'\n",
    "#proxy = choice(proxy_check()) /gpu-specs/\n",
    "source = requests.get(tech_url , timeout=10)\n",
    "#source = proxy_check(tech_url, proxy_list)\n",
    "#proxy_index += 1\n",
    "print(source.status_code)\n",
    "src = source.content\n",
    "soup = bs(src,'lxml')\n",
    "\n",
    "links = []\n",
    "for i in soup.find('div', class_=\"table-wrapper\").find_all('tr'):\n",
    "    next_page = i.find('a')\n",
    "    if next_page != None:\n",
    "        #time.sleep(random.randint(0,3)).strip('/gpu-specs/').strip('/gpu-spec/')\n",
    "        page_list = next_page.attrs['href']\n",
    "        print(page_list)\n",
    "        links.append(tech_url.strip('/gpu-spec/')+page_list)\n",
    "     \n",
    "    \n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7da7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b66c60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a952d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "title_list = []\n",
    "col_label_list = []\n",
    "data_list = []\n",
    "while True:\n",
    "    for i in links:\n",
    "        if counter <= len(links):\n",
    "            time.sleep(random.randint(3,5))\n",
    "            source = proxy_check(tech_url, proxy_list)\n",
    "            #source = requests.get(i, proxies = proxy)\n",
    "            print(source.status_code)\n",
    "            #if source.status_code == 429:\n",
    "                #proxy_index += 1\n",
    "            src = source.content\n",
    "            soup = bs(src,'lxml')\n",
    "            print(f\"-----------{counter}----------\")\n",
    "            #title = soup.find('h1', class_=\"gpudb-name\").text\n",
    "            #title_list.append(title)\n",
    "            #print(title)\n",
    "            stats = soup.find('div').find_all('dl')\n",
    "            for j in stats:\n",
    "                column_label = j.find('dt').text\n",
    "                col_label_list.append(column_label)\n",
    "                data = j.find('dd').text.replace('\\n',\"\").replace('\\t',\"\")\n",
    "                data_list.append(data)\n",
    "            counter+=1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f09093",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_list)\n",
    "print(col_label_list)\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d535a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title = soup2.find('h1', class_=\"gpudb-name\").text\n",
    "#specs = soup2.find('dd', class_=\"gpudb-specs-large__value\")\n",
    "specs1 = soup2.find('div').find_all('dl')\n",
    "#specs2 = soup2.find('dl', class_=\"clearfix\").find_all('div')\n",
    "#specs1 = soup2.find('dl', class_=\"gpudb-specs-large\").find_all('div')\n",
    "print(title)\n",
    "list_info = []\n",
    "list_info_heading = []\n",
    "for spec1 in specs1:\n",
    "    basic_info = spec1.find('dd').text.replace('\\n',\"\").replace('\\t',\"\")\n",
    "    basic_heading = spec1.find('dt').text\n",
    "    list_info.append(basic_info)\n",
    "    list_info_heading.append(basic_heading)\n",
    "    \n",
    "print(list_info_heading)\n",
    "#print(list_info)\n",
    "\n",
    "#for i in list_info:\n",
    "    #print(list_info[i].strip(','))\n",
    "\n",
    "keep = []\n",
    "pattern_sub = ',|,'\n",
    "pattern_find = '\\d+\\.*\\d*|$'\n",
    "#pattern_find = '\\d+|$'\n",
    "\n",
    "test_list = [re.sub(pattern_sub, '', i) for i in list_info]\n",
    "test_list2 = [re.findall(pattern_find, i)[0] for i in test_list]\n",
    "# re.findall('\\d+|$', 'aa33bbb44')[0]\n",
    "#for i in list_info:'/^[+-]?([0-9]+\\.?[0-9]*|\\.[0-9]+)$/'\n",
    "#x = [re.findall(pattern, ' ', i) for i in list_info]\n",
    "#x = list([re.findall(pattern, list_info) for i in list_info])\n",
    "#pattern = '[0-9]'\n",
    "#test_list = [re.sub(pattern, '', i) for i in list_info]\n",
    "\n",
    "#string = ''.join(list_info)\n",
    "#match = re.findall(pattern, string)\n",
    "# Pattern.compile(\"(^|\\\\s)([0-9]+)($|\\\\s)\");\n",
    "# match = re.findall(r\"\\([A-Z]+\\)|Pool ID\", string)\n",
    "\n",
    "  \n",
    "\n",
    "#print(x)    \n",
    "    \n",
    "#print(x)\n",
    "#print(keep)\n",
    "\n",
    "    \n",
    "#print(title)\n",
    "\n",
    "    \n",
    "    #test = spec.find('dd', class_='gpudb-specs-large__value').text \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n",
    "    #print(list_value) ,class_='gpudb-specs-large__value'\n",
    "    # ,class_='gpudb-specs-large__title' replace(\"geeks\", \"GeeksforGeeks\", 3))\n",
    "\n",
    "#print(specs1)   .strip('\\t').strip('\\n').strip()\n",
    "    \n",
    "#movies = soup.find('tbody', class_=\"lister-list\").find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860af2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb77de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(names=io.StringIO('\\n'.join(list_info_heading)), delim_whitespace=False)\n",
    "#df = pd.DataFrame(list_info_heading)\n",
    "#pd.DataFrame.from_dict(data)\n",
    "temp_dict = {}\n",
    "for key, value in zip(list_info_heading, test_list2):\n",
    "    temp_dict[key] = value  \n",
    "        \n",
    "df = pd.DataFrame(temp_dict, index=[0])\n",
    "\n",
    "\n",
    "#print(list(df.columns))\n",
    "drop_list = ['GPU Name', 'GPU Variant', 'Architecture', 'Foundry', 'Production', 'Slot Width', \n",
    "             'Length', 'Width', 'Height', 'Outputs', 'DirectX', 'OpenGL', 'OpenCL', 'Vulkan', \n",
    "             'Shader Model','Power Connectors','Part Number','Suggested PSU','Predecessor',\n",
    "             'L0 Cache', 'L1 Cache', 'L2 Cache', 'L3 Cache']\n",
    "#columns =['C', 'D']\n",
    "df['Graphics Processor'] = title\n",
    "df = df.drop(columns = drop_list)\n",
    "#df = df.convert_dtypes()\n",
    "#df = df.convert_dtypes().dtypes\n",
    "#df = pd.DataFrame(temp_dict)\n",
    "#df = df.append(temp_dict, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7d6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ecade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8bb3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    source = requests.get('https://www.imdb.com/chart/top/')\n",
    "    source.raise_for_status()\n",
    "    soup = BeautifulSoup(source.text,'html.parser')\n",
    "    movies = soup.find('tbody', class_=\"lister-list\").find_all('tr')\n",
    "    print(movies)\n",
    "    df = pd.DataFrame(columns=['Rank','Name','Year','Rating'])\n",
    "    temp_dict = {}\n",
    "    temp_list = []\n",
    "    \n",
    "    for movie in movies:\n",
    "        name = movie.find('td', class_='titleColumn').a.text\n",
    "        rank = movie.find('td', class_='titleColumn').get_text(strip=True).split('.')[0]\n",
    "        year = movie.find('td', class_='titleColumn').span.text.strip('()')\n",
    "        rating = movie.find('td', class_='ratingColumn imdbRating').strong.text\n",
    "        print(name)\n",
    "        list_key = ['Rank','Name','Year','Rating']\n",
    "        list_value = [rank, name, year, rating]\n",
    "        \n",
    "        for key, value in zip(list_key, list_value):\n",
    "            temp_dict[key] = value\n",
    "            \n",
    "        df = df.append(temp_dict, ignore_index=True, sort=False)\n",
    "   \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0bbb07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79b03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
